class FTransEModule(BaseModule):
    def __init__(self, n_ent, n_rel, args):
        super(FTransEModule, self).__init__(n_ent, n_rel, args)
        self.rel_embed = nn.Embedding(n_rel, args.hidden_dim)
        self.ent_embed = nn.Embedding(n_ent, args.hidden_dim)
        self.init_weight()

    def forward(self, head, tail, rela):
        shape = head.size()
        head = head.contiguous().view(-1)
        tail = tail.contiguous().view(-1)
        rela = rela.contiguous().view(-1)
        head_embed = F.normalize(self.ent_embed(head),2,-1)
        tail_embed = F.normalize(self.ent_embed(tail),2,-1)
        rela_embed = self.rel_embed(rela)
        print(head_embed) 
        h_r = head_embed + rela_embed
        t_r = tail_embed - rela_embed
        
        h_r_T = h_r.transpose(0, 1)
        h_T = head_embed.transpose(0, 1)
        
        h_r_T_dot_t = torch.matmul(h_r_T, tail_embed)
        h_T_dot_t_r = torch.matmul(h_T, t_r)
        print(h_r_T_dot_t.size())
        print(h_T_dot_t_r.size())
        shape1 = h_r_T_dot_t.size()
        abc = h_r_T_dot_t + h_T_dot_t_r
        abc = abc.reshape([223370,100])
        return torch.norm((h_r_T_dot_t + h_T_dot_t_r).reshape([3370,100]), p=self.p, dim=-1).view(shape)

    def dist(self, head, tail, rela):
        return self.forward(head, tail, rela)

    def score(self, head, tail, rela):
        return self.forward(head, tail, rela)

    def prob_logit(self, head, tail, rela):
        return -self.forward(head, tail, rela) / self.temp
